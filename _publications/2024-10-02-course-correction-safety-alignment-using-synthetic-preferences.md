---
title: "Course-Correction: Safety Alignment Using Synthetic Preferences"
collection: publications
category: conferences
permalink: /publication/2024-10-02-course-correction-safety-alignment-using-synthetic-preferences
excerpt: 'The risk of harmful contents generated by large language models (LLMs) becomes a critical concern. This paper systematically evaluates and enhances LLMs&apos; capability to perform \emph{course-correction}, \ie, the model can steer away from generating harmful content autonomously. First, we introduce the C-Eval benchmark for quantitative assessment and analyze 10 popular LLMs, revealing varying proficiency of current safety-tuned LLMs in course-correction. To improve, we propose fine-tuning LLMs with preference learning, emphasizing the preference for timely course-correction. Using an automated pipeline, we create C-Syn, a synthetic C-Syn with 750K pairwise preferences, to teach models the concept of timely course-correction through data-driven learning. Experiments on \textsc{Llama2-Chat 7B} and \textsc{Qwen2 7B} show that our method effectively enhances course-correction skills without affecting general performance. Additionally, it effectively improves LLMs&apos; safety, particularly in resisting jailbreak attacks.'
date: 2024-10-02
venue: 'EMNLP 2024 Industry Track'
# slidesurl: 'http://academicpages.github.io/files/slides2.pdf'
paperurl: 'https://arxiv.org/abs/2407.16637'
citation: 'Xu, R., Cai, Y., Zhou, Z., Gu, R., Weng, H., Liu, Y., ... &amp; Qiu, H. (2024). Course-Correction: Safety Alignment Using Synthetic Preferences. arXiv preprint arXiv:2407.16637.'
---
The risk of harmful contents generated by large language models (LLMs) becomes a critical concern. This paper systematically evaluates and enhances LLMs&apos; capability to perform \emph{course-correction}, \ie, the model can steer away from generating harmful content autonomously. First, we introduce the C-Eval benchmark for quantitative assessment and analyze 10 popular LLMs, revealing varying proficiency of current safety-tuned LLMs in course-correction. To improve, we propose fine-tuning LLMs with preference learning, emphasizing the preference for timely course-correction. Using an automated pipeline, we create C-Syn, a synthetic C-Syn with 750K pairwise preferences, to teach models the concept of timely course-correction through data-driven learning. Experiments on \textsc{Llama2-Chat 7B} and \textsc{Qwen2 7B} show that our method effectively enhances course-correction skills without affecting general performance. Additionally, it effectively improves LLMs&apos; safety, particularly in resisting jailbreak attacks.

[Download slides here](http://academicpages.github.io/files/slides2.pdf)

[Download paper here](https://arxiv.org/abs/2407.16637)

Recommended citation: Xu, R., Cai, Y., Zhou, Z., Gu, R., Weng, H., Liu, Y., ... & Qiu, H. (2024). Course-Correction: Safety Alignment Using Synthetic Preferences. arXiv preprint arXiv:2407.16637.