---
title: "Course-Correction: Safety Alignment Using Synthetic Preferences"
collection: publications
category: conferences
permalink: /publication/2024-10-02-Course-Correction
excerpt: 'The risk of harmful contents generated by large language models (LLMs) becomes a critical concern. This paper systematically evaluates and enhances LLMs' capability to perform \emph{course-correction}, \ie, the model can steer away from generating harmful content autonomously. First, we introduce the C-Eval benchmark for quantitative assessment and analyze 10 popular LLMs, revealing varying proficiency of current safety-tuned LLMs in course-correction. To improve, we propose fine-tuning LLMs with preference learning, emphasizing the preference for timely course-correction. Using an automated pipeline, we create C-Syn, a synthetic C-Syn with 750K pairwise preferences, to teach models the concept of timely course-correction through data-driven learning. Experiments on \textsc{Llama2-Chat 7B} and \textsc{Qwen2 7B} show that our method effectively enhances course-correction skills without affecting general performance. Additionally, it effectively improves LLMs' safety, particularly in resisting jailbreak attacks.'
date: 2024-10-02
venue: 'EMNLP 2024 Industry Track'
slidesurl: ''
paperurl: 'https://arxiv.org/abs/2407.16637'
citation: 'Xu, R., Cai, Y., Zhou, Z., Gu, R., Weng, H., Liu, Y., ... & Qiu, H. (2024). Course-Correction: Safety Alignment Using Synthetic Preferences. arXiv preprint arXiv:2407.16637.'
---

The contents above will be part of a list of publications, if the user clicks the link for the publication than the contents of section will be rendered as a full page, allowing you to provide more information about the paper for the reader. When publications are displayed as a single page, the contents of the above "citation" field will automatically be included below this section in a smaller font.