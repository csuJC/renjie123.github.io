pub_date	title	venue	excerpt	citation	url_slug	paper_url	slides_url
2009-10-01	Paper Title Number 1	Journal 1	This paper is about the number 1. The number 2 is left for future work.	Your Name, You. (2009). "Paper Title Number 1." <i>Journal 1</i>. 1(1).	paper-title-number-1	http://academicpages.github.io/files/paper1.pdf	http://academicpages.github.io/files/slides1.pdf
2024-10-02	Course-Correction: Safety Alignment Using Synthetic Preferences.   EMNLP 2024 Industry Track.   The risk of harmful contents generated by large language models (LLMs) becomes a critical concern. This paper systematically evaluates and enhances LLMs' capability to perform \emph{course-correction}, \ie, the model can steer away from generating harmful content autonomously. First, we introduce the C-Eval benchmark for quantitative assessment and analyze 10 popular LLMs, revealing varying proficiency of current safety-tuned LLMs in course-correction. To improve, we propose fine-tuning LLMs with preference learning, emphasizing the preference for timely course-correction. Using an automated pipeline, we create C-Syn, a synthetic C-Syn with 750K pairwise preferences, to teach models the concept of timely course-correction through data-driven learning. Experiments on \textsc{Llama2-Chat 7B} and \textsc{Qwen2 7B} show that our method effectively enhances course-correction skills without affecting general performance. Additionally, it effectively improves LLMs' safety, particularly in resisting jailbreak attacks.	Your Name, You. (2024). "Course-Correction: Safety Alignment Using Synthetic Preferences." <i>EMNLP 2024 Industry Track</i>. 1(1).	course-correction-safety-alignment-using-synthetic-preferences	http://academicpages.github.io/files/paper1.pdf	http://academicpages.github.io/files/slides1.pdf